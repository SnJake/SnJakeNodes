import torch
import re

class TokenWeightChecker:
    """
    –ù–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ "–≤–µ—Å–∞" —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–æ–¥–µ–ª–∏ CLIP.
    –ü–æ–¥ "–≤–µ—Å–æ–º" –ø–æ–Ω–∏–º–∞–µ—Ç—Å—è L2-–Ω–æ—Ä–º–∞ (–≤–µ–ª–∏—á–∏–Ω–∞) –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–∞.
    –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ç–æ, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–µ—Å–µ—Ç –±–æ–ª—å—à–µ "–∏–∑—É—á–µ–Ω–Ω–æ–π" –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "tokens": ("STRING", {
                    "multiline": True,
                    "default": "cat, dog, astronaut, a beautiful landscape"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("weights_string",)
    FUNCTION = "check_token_weights"
    CATEGORY = "üòé SnJake/Utils"

    def check_token_weights(self, model, tokens):
        output_lines = []
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ CLIP –º–æ–¥–µ–ª–∏. 
        # –í ComfyUI –æ–±—ä–µ–∫—Ç model —è–≤–ª—è–µ—Ç—Å—è ModelPatcher, –∫–æ—Ç–æ—Ä—ã–π –æ–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å.
        if not hasattr(model, 'model') or not hasattr(model.model, 'clip'):
            return ("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ CLIP –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.",)

        clip = model.model.clip
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        try:
            tokenizer = clip.tokenizer
            embedding_layer = clip.transformer.text_model.embeddings.token_embedding
            embeddings = embedding_layer.weight
        except AttributeError:
            return ("–û—à–∏–±–∫–∞: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ CLIP-–º–æ–¥–µ–ª–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π. –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.",)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–≤–µ–¥–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –∏ –Ω–æ–≤—ã–º —Å—Ç—Ä–æ–∫–∞–º
        token_list = [t.strip() for t in re.split('[,\\n]', tokens) if t.strip()]

        if not token_list:
            return ("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ.",)

        output_lines.append(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Å–æ–≤ –¥–ª—è {len(token_list)} —Ç–æ–∫–µ–Ω–æ–≤:")
        output_lines.append("-" * 20)

        for token_str in token_list:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É
            # encode –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ (e.g., [49406, ID, 49407])
            token_ids = tokenizer.encode(token_str)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ –≤–≤–æ–¥ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
            if len(token_ids) != 3:
                # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π
                sub_tokens_info = []
                # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞ [1:-1]
                for token_id in token_ids[1:-1]:
                    sub_token_str = tokenizer.decode([token_id])
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–µ —è–≤–ª—è–µ—Ç—Å—è UNK (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º)
                    if token_id == tokenizer.unk_token_id:
                        weight_info = "[–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω]"
                    else:
                        token_vector = embeddings[token_id]
                        weight = torch.linalg.norm(token_vector).item()
                        weight_info = f"{weight:.4f}"
                    sub_tokens_info.append(f"'{sub_token_str}' ({weight_info})")
                
                output_lines.append(f"'{token_str}' -> {' '.join(sub_tokens_info)}")

            else:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                token_id = token_ids[1]
                if token_id == tokenizer.unk_token_id:
                    output_lines.append(f"'{token_str}': [–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω]")
                else:
                    token_vector = embeddings[token_id]
                    weight = torch.linalg.norm(token_vector).item()
                    output_lines.append(f"'{token_str}': {weight:.4f}")
        
        # –°–æ–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫
        result_string = "\n".join(output_lines)
        
        return (result_string,)

