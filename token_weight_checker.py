import torch
import re

class TokenWeightChecker:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ "–≤–µ—Å–∞" —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–æ–¥–µ–ª—è—Ö CLIP.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ
    —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ SDXL, SD3, Flux).
    –ü–æ–¥ "–≤–µ—Å–æ–º" –ø–æ–Ω–∏–º–∞–µ—Ç—Å—è L2-–Ω–æ—Ä–º–∞ (–≤–µ–ª–∏—á–∏–Ω–∞) –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–∞.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "clip": ("CLIP",),
                "tokens": ("STRING", {
                    "multiline": True,
                    "default": "cat, dog, astronaut, a beautiful landscape"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("weights_string",)
    FUNCTION = "check_token_weights"
    CATEGORY = "üòé SnJake/Utils"

    def _process_encoder(self, name, encoder, token_list):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞."""
        lines = [f"--- –ê–Ω–∞–ª–∏–∑ –¥–ª—è: {name} ---"]
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            tokenizer = encoder.tokenizer
            
            # –£ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            if hasattr(encoder, 'transformer') and hasattr(encoder.transformer, 'text_model'): # –î–ª—è OpenCLIP (clip-l, clip-g)
                embeddings = encoder.transformer.text_model.embeddings.token_embedding.weight
            elif hasattr(encoder, 'transformer') and hasattr(encoder.transformer, 'get_input_embeddings'): # –î–ª—è T5
                embeddings = encoder.transformer.get_input_embeddings().weight
            else:
                lines.append("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏.")
                return lines

            for token_str in token_list:
                token_ids = tokenizer.encode(token_str)
                meaningful_tokens = token_ids[1:-1] # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞

                if not meaningful_tokens:
                    lines.append(f"'{token_str}': [–ù–µ —É–¥–∞–ª–æ—Å—å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å]")
                    continue

                if len(meaningful_tokens) > 1:
                    sub_tokens_info = []
                    for token_id in meaningful_tokens:
                        if token_id >= len(embeddings):
                            sub_tokens_info.append(f"id:{token_id}[–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                            continue
                        
                        sub_token_str = tokenizer.decode([token_id])
                        if token_id == getattr(tokenizer, 'unk_token_id', -1):
                            weight_info = "[UNK]"
                        else:
                            weight = torch.linalg.norm(embeddings[token_id]).item()
                            weight_info = f"{weight:.4f}"
                        sub_tokens_info.append(f"'{sub_token_str}'({weight_info})")
                    lines.append(f"'{token_str}' -> {' '.join(sub_tokens_info)}")
                else:
                    token_id = meaningful_tokens[0]
                    if token_id >= len(embeddings):
                        lines.append(f"'{token_str}': id:{token_id}[–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                        continue
                        
                    if token_id == getattr(tokenizer, 'unk_token_id', -1):
                        lines.append(f"'{token_str}': [–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω]")
                    else:
                        weight = torch.linalg.norm(embeddings[token_id]).item()
                        lines.append(f"'{token_str}': {weight:.4f}")
            
        except Exception as e:
            lines.append(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ '{name}': {e}")
            import traceback
            lines.append(traceback.format_exc())
            
        return lines

    def check_token_weights(self, clip, tokens):
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–Ω–∫–æ–¥–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –æ–±—ä–µ–∫—Ç–µ clip
        encoders_to_check = {}
        possible_encoder_names = ['clip_l', 'clip_g', 't5xxl']
        
        for name in possible_encoder_names:
            if hasattr(clip, name) and getattr(clip, name) is not None:
                encoders_to_check[name.upper()] = getattr(clip, name)
        
        # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ,
        # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Å–∞–º 'clip' —è–≤–ª—è–µ—Ç—Å—è —ç–Ω–∫–æ–¥–µ—Ä–æ–º (–¥–ª—è SD 1.5)
        if not encoders_to_check:
            if hasattr(clip, 'tokenizer') and hasattr(clip, 'transformer'):
                 encoders_to_check['CLIP'] = clip
            else:
                return ("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É CLIP. –≠—Ç–æ –Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π CLIP-–æ–±—ä–µ–∫—Ç –∏ –Ω–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä (SDXL/SD3).",)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–≤–µ–¥–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        token_list = [t.strip() for t in re.split('[,\\n]', tokens) if t.strip()]
        if not token_list:
            return ("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.",)

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Å–µ–º —ç–Ω–∫–æ–¥–µ—Ä–∞–º
        final_output = []
        for name, encoder in encoders_to_check.items():
            result_lines = self._process_encoder(name, encoder, token_list)
            final_output.extend(result_lines)
            final_output.append("") # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è

        return ("\n".join(final_output),)
