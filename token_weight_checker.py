import torch
import re
import traceback

class TokenWeightChecker:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ "–≤–µ—Å–∞" —Ç–æ–∫–µ–Ω–æ–≤.
    –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –≤–Ω—É—Ç—Ä–∏ –æ–±—ä–µ–∫—Ç–∞ CLIP,
    –≤–∫–ª—é—á–∞—è –æ–±—Ä–∞–±–æ—Ç–∫—É –æ–±—ä–µ–∫—Ç–æ–≤-–æ–±–µ—Ä—Ç–æ–∫ (wrappers), —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ 
    —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π —Å SD1.5, SDXL, SD3, Flux –∏ –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "clip": ("CLIP",),
                "tokens": ("STRING", {
                    "multiline": True,
                    "default": "cat, dog, astronaut, a beautiful landscape"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("weights_string",)
    FUNCTION = "check_token_weights"
    CATEGORY = "üòé SnJake/Utils"

    def _process_encoder(self, name, encoder, token_list):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞."""
        lines = [f"--- –ê–Ω–∞–ª–∏–∑ –¥–ª—è: {name} ---"]
        
        try:
            tokenizer = getattr(encoder, 'tokenizer', None)
            if not tokenizer:
                lines.append("–û—à–∏–±–∫–∞: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —ç—Ç–æ–º —ç–Ω–∫–æ–¥–µ—Ä–µ.")
                return lines

            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            if hasattr(encoder, 'transformer') and hasattr(encoder.transformer, 'text_model') and hasattr(encoder.transformer.text_model, 'embeddings'):
                embeddings = encoder.transformer.text_model.embeddings.token_embedding.weight
            elif hasattr(encoder, 'transformer') and hasattr(encoder.transformer, 'get_input_embeddings'):
                embeddings = encoder.transformer.get_input_embeddings().weight
            else:
                lines.append("–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏.")
                return lines

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º ID —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            special_ids = set()
            for token_name in ['bos_token_id', 'eos_token_id', 'pad_token_id', 'unk_token_id']:
                token_id = getattr(tokenizer, token_name, None)
                if token_id is not None:
                    special_ids.add(token_id)

            for token_str in token_list:
                if not token_str.strip():
                    continue

                token_ids = tokenizer.encode(token_str)
                meaningful_tokens = [tid for tid in token_ids if tid not in special_ids]
                
                if not meaningful_tokens:
                    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å, –≤–æ–∑–º–æ–∂–Ω–æ —ç—Ç–æ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω
                    # –∏–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å. –í—ã–≤–µ–¥–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
                    decoded_tokens = [f"'{tokenizer.decode([tid])}'({tid})" for tid in token_ids]
                    lines.append(f"'{token_str}' -> [–ù–µ—Ç –∑–Ω–∞—á–∞—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤] Raw: {' '.join(decoded_tokens)}")
                    continue

                if len(meaningful_tokens) > 1:
                    sub_tokens_info = []
                    for token_id in meaningful_tokens:
                        if token_id >= len(embeddings):
                            sub_tokens_info.append(f"id:{token_id}[–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                            continue
                        
                        sub_token_str = tokenizer.decode([token_id])
                        weight = torch.linalg.norm(embeddings[token_id]).item()
                        sub_tokens_info.append(f"'{sub_token_str}'({weight:.4f})")
                    lines.append(f"'{token_str}' -> {' '.join(sub_tokens_info)}")
                else:
                    token_id = meaningful_tokens[0]
                    if token_id >= len(embeddings):
                        lines.append(f"'{token_str}': id:{token_id}[–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                        continue
                    
                    weight = torch.linalg.norm(embeddings[token_id]).item()
                    lines.append(f"'{token_str}': {weight:.4f}")
            
        except Exception as e:
            lines.append(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ '{name}': {e}")
            lines.append(traceback.format_exc())
            
        return lines

    def check_token_weights(self, clip, tokens):
        encoders_to_check = {}
        
        # –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∏–Ω—Å–ø–µ–∫—Ü–∏–∏. –ù–∞—á–∏–Ω–∞–µ–º —Å —Å–∞–º–æ–≥–æ clip
        # –∏ –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ .target, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.
        objects_to_inspect = [clip]
        if hasattr(clip, 'target'):
            objects_to_inspect.append(clip.target)

        for obj in objects_to_inspect:
            if obj is None: continue
            # –ò—â–µ–º —ç–Ω–∫–æ–¥–µ—Ä—ã –∫–∞–∫ –∞—Ç—Ä–∏–±—É—Ç—ã –æ–±—ä–µ–∫—Ç–∞
            for attr_name in dir(obj):
                if attr_name.startswith('_'):
                    continue
                
                try:
                    attr_value = getattr(obj, attr_name)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂ –ª–∏ –∞—Ç—Ä–∏–±—É—Ç –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —ç–Ω–∫–æ–¥–µ—Ä
                    if hasattr(attr_value, 'tokenizer') and hasattr(attr_value, 'transformer'):
                         # –ù–µ –¥–æ–±–∞–≤–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                        if id(attr_value) not in [id(v) for v in encoders_to_check.values()]:
                            encoders_to_check[attr_name.upper()] = attr_value
                except Exception:
                    continue
        
        # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–∏—Å–∫–æ–≤ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–∞–º 'clip' —ç–Ω–∫–æ–¥–µ—Ä–æ–º
        if not encoders_to_check:
            if hasattr(clip, 'tokenizer') and hasattr(clip, 'transformer'):
                encoders_to_check['CLIP'] = clip

        if not encoders_to_check:
            return ("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π CLIP-–æ–±—ä–µ–∫—Ç.",)

        token_list = [t.strip() for t in re.split('[,\\n]', tokens) if t.strip()]
        if not token_list:
            return ("–í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.",)

        final_output = []
        for name, encoder in sorted(encoders_to_check.items()):
            result_lines = self._process_encoder(name, encoder, token_list)
            final_output.extend(result_lines)
            final_output.append("")

        return ("\n".join(final_output),)
