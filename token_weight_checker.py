import torch
import re
import traceback

class TokenWeightChecker:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –Ω–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ "–≤–µ—Å–∞" —Ç–æ–∫–µ–Ω–æ–≤.
    –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã –≤–Ω—É—Ç—Ä–∏ –æ–±—ä–µ–∫—Ç–∞ CLIP,
    —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–π —Å SD1.5, SDXL, SD3, Flux –∏ –¥—Ä—É–≥–∏–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "clip": ("CLIP",),
                "tokens": ("STRING", {
                    "multiline": True,
                    "default": "cat, dog, astronaut, a beautiful landscape"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("weights_string",)
    FUNCTION = "check_token_weights"
    CATEGORY = "üòé SnJake/Utils"

    def _process_encoder(self, name, encoder, token_list):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞."""
        lines = [f"--- –ê–Ω–∞–ª–∏–∑ –¥–ª—è: {name} ---"]
        
        try:
            tokenizer = getattr(encoder, 'tokenizer', None)
            if not tokenizer:
                lines.append("–û—à–∏–±–∫–∞: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω.")
                return lines

            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            if hasattr(encoder, 'transformer') and hasattr(encoder.transformer, 'text_model') and hasattr(encoder.transformer.text_model, 'embeddings'):
                embeddings = encoder.transformer.text_model.embeddings.token_embedding.weight
            elif hasattr(encoder, 'transformer') and hasattr(encoder.transformer, 'get_input_embeddings'):
                embeddings = encoder.transformer.get_input_embeddings().weight
            else:
                lines.append("–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏.")
                return lines

            for token_str in token_list:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                if not token_str.strip():
                    continue

                token_ids = tokenizer.encode(token_str)
                # –£–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç ID 49406 –∏ 49407
                meaningful_tokens = [tid for tid in token_ids if tid not in [tokenizer.bos_token_id, tokenizer.eos_token_id, getattr(tokenizer, 'pad_token_id', -1)]]
                # Fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
                if not meaningful_tokens and len(token_ids) > 2:
                    meaningful_tokens = token_ids[1:-1]
                
                if not meaningful_tokens:
                    lines.append(f"'{token_str}': [–ù–µ —É–¥–∞–ª–æ—Å—å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å]")
                    continue

                if len(meaningful_tokens) > 1:
                    sub_tokens_info = []
                    for token_id in meaningful_tokens:
                        if token_id >= len(embeddings):
                            sub_tokens_info.append(f"id:{token_id}[–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                            continue
                        
                        sub_token_str = tokenizer.decode([token_id])
                        if token_id == getattr(tokenizer, 'unk_token_id', -1):
                            weight_info = "[UNK]"
                        else:
                            weight = torch.linalg.norm(embeddings[token_id]).item()
                            weight_info = f"{weight:.4f}"
                        sub_tokens_info.append(f"'{sub_token_str}'({weight_info})")
                    lines.append(f"'{token_str}' -> {' '.join(sub_tokens_info)}")
                else:
                    token_id = meaningful_tokens[0]
                    if token_id >= len(embeddings):
                        lines.append(f"'{token_str}': id:{token_id}[–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                        continue
                        
                    if token_id == getattr(tokenizer, 'unk_token_id', -1):
                        lines.append(f"'{token_str}': [–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω]")
                    else:
                        weight = torch.linalg.norm(embeddings[token_id]).item()
                        lines.append(f"'{token_str}': {weight:.4f}")
            
        except Exception as e:
            lines.append(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–Ω–∫–æ–¥–µ—Ä–∞ '{name}': {e}")
            lines.append(traceback.format_exc())
            
        return lines

    def check_token_weights(self, clip, tokens):
        encoders_to_check = {}

        # 1. –ò–Ω—Ç—Ä–æ—Å–ø–µ–∫—Ü–∏—è: –∏—â–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —ç–Ω–∫–æ–¥–µ—Ä—ã
        for attr_name in dir(clip):
            if attr_name.startswith('_'):
                continue
            
            try:
                attr_value = getattr(clip, attr_name)
                if hasattr(attr_value, 'tokenizer') and hasattr(attr_value, 'transformer'):
                    encoders_to_check[attr_name.upper()] = attr_value
            except Exception:
                continue

        # 2. –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–∞–º –æ–±—ä–µ–∫—Ç clip
        if not encoders_to_check:
            if hasattr(clip, 'tokenizer') and hasattr(clip, 'transformer'):
                encoders_to_check['CLIP'] = clip
        
        # 3. –ï—Å–ª–∏ —Å–Ω–æ–≤–∞ –Ω–µ—É–¥–∞—á–∞, —Å–æ–æ–±—â–∞–µ–º –æ–± –æ—à–∏–±–∫–µ
        if not encoders_to_check:
            error_msg = ("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞.\n"
                         "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π CLIP-–æ–±—ä–µ–∫—Ç.")
            return (error_msg,)

        token_list = [t.strip() for t in re.split('[,\\n]', tokens) if t.strip()]
        if not token_list:
            return ("–í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.",)

        final_output = []
        for name, encoder in encoders_to_check.items():
            result_lines = self._process_encoder(name, encoder, token_list)
            final_output.extend(result_lines)
            final_output.append("")

        return ("\n".join(final_output),)
