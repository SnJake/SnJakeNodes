import torch
import re

class TokenWeightChecker:
    """
    –ù–æ–¥–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ "–≤–µ—Å–∞" —Ç–æ–∫–µ–Ω–æ–≤ –≤ –º–æ–¥–µ–ª–∏ CLIP.
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ CLIP, –∞ –Ω–µ MODEL.
    –ü–æ–¥ "–≤–µ—Å–æ–º" –ø–æ–Ω–∏–º–∞–µ—Ç—Å—è L2-–Ω–æ—Ä–º–∞ (–≤–µ–ª–∏—á–∏–Ω–∞) –≤–µ–∫—Ç–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Ç–æ–∫–µ–Ω–∞.
    –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ç–æ, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–µ—Å–µ—Ç –±–æ–ª—å—à–µ "–∏–∑—É—á–µ–Ω–Ω–æ–π" –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                # –ò–ó–ú–ï–ù–ï–ù–û: –¢–µ–ø–µ—Ä—å –Ω–æ–¥–∞ –Ω–∞–ø—Ä—è–º—É—é –ø—Ä–∏–Ω–∏–º–∞–µ—Ç CLIP
                "clip": ("CLIP",),
                "tokens": ("STRING", {
                    "multiline": True,
                    "default": "cat, dog, astronaut, a beautiful landscape"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("weights_string",)
    FUNCTION = "check_token_weights"
    CATEGORY = "üòé SnJake/Utils"

    # –ò–ó–ú–ï–ù–ï–ù–û: –§—É–Ω–∫—Ü–∏—è —Ç–µ–ø–µ—Ä—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç 'clip' –≤–º–µ—Å—Ç–æ 'model'
    def check_token_weights(self, clip, tokens):
        output_lines = []
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –æ–±—ä–µ–∫—Ç–∞ CLIP
        try:
            tokenizer = clip.tokenizer
            embedding_layer = clip.transformer.text_model.embeddings.token_embedding
            embeddings = embedding_layer.weight
        except AttributeError:
            return ("–û—à–∏–±–∫–∞: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ CLIP-–º–æ–¥–µ–ª–∏ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–π. –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.",)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–≤–µ–¥–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –∏ –Ω–æ–≤—ã–º —Å—Ç—Ä–æ–∫–∞–º
        token_list = [t.strip() for t in re.split('[,\\n]', tokens) if t.strip()]

        if not token_list:
            return ("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏. –í–≤–µ–¥–∏—Ç–µ —Ç–æ–∫–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø–æ–ª–µ.",)

        output_lines.append(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Å–æ–≤ –¥–ª—è {len(token_list)} —Ç–æ–∫–µ–Ω–æ–≤:")
        output_lines.append("-" * 20)

        for token_str in token_list:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É
            # .encode() –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç–æ–∫–µ–Ω—ã –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ (e.g., [49406, ID, 49407] –¥–ª—è SD1.5)
            token_ids = tokenizer.encode(token_str)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ –≤–≤–æ–¥ –∏–∑ –æ–¥–Ω–æ–≥–æ "–∑–Ω–∞—á–∏–º–æ–≥–æ" —Ç–æ–∫–µ–Ω–∞ (–∏—Å–∫–ª—é—á–∞—è start/end)
            # –î–ª—è SDXL –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            meaningful_tokens = token_ids[1:-1]
            if not meaningful_tokens:
                 output_lines.append(f"'{token_str}': [–ù–µ —É–¥–∞–ª–æ—Å—å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å]")
                 continue

            if len(meaningful_tokens) > 1:
                # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π
                sub_tokens_info = []
                for token_id in meaningful_tokens:
                    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ ID –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Å–ª–æ–≤–∞—Ä—è
                    if token_id >= len(embeddings):
                        sub_tokens_info.append(f"ID {token_id} [–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                        continue
                    
                    sub_token_str = tokenizer.decode([token_id])
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ–∫–µ–Ω –Ω–µ —è–≤–ª—è–µ—Ç—Å—è UNK (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º)
                    if token_id == tokenizer.unk_token_id:
                        weight_info = "[–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω]"
                    else:
                        token_vector = embeddings[token_id]
                        weight = torch.linalg.norm(token_vector).item()
                        weight_info = f"{weight:.4f}"
                    sub_tokens_info.append(f"'{sub_token_str}' ({weight_info})")
                
                output_lines.append(f"'{token_str}' -> {' '.join(sub_tokens_info)}")

            else:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                token_id = meaningful_tokens[0]
                if token_id >= len(embeddings):
                    output_lines.append(f"'{token_str}': ID {token_id} [–≤–Ω–µ —Å–ª–æ–≤–∞—Ä—è]")
                    continue

                if token_id == tokenizer.unk_token_id:
                    output_lines.append(f"'{token_str}': [–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω]")
                else:
                    token_vector = embeddings[token_id]
                    weight = torch.linalg.norm(token_vector).item()
                    output_lines.append(f"'{token_str}': {weight:.4f}")
        
        # –°–æ–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –±–ª–æ–∫
        result_string = "\n".join(output_lines)
        
        return (result_string,)
