import torch
import numpy as np
import math
import comfy.utils
import comfy.samplers
import comfy.sample
import nodes
import latent_preview
from PIL import Image
import torchvision.transforms.functional as F
from scipy.ndimage import gaussian_filter, label, find_objects, grey_dilation, binary_closing, binary_fill_holes

# region: --- Helper functions from InpaintStitchImproved ---
# –ú—ã –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏ —Ö–æ—Ä–æ—à–æ –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏.
# –≠—Ç–æ –ª—É—á—à–∏–π –ø–æ–¥—Ö–æ–¥, —á–µ–º –ø–∏—Å–∞—Ç—å —Å–≤–æ–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.

def rescale_i(samples, width, height, algorithm: str):
    """Rescales an image tensor (BHWC)."""
    samples = samples.movedim(-1, 1)
    # getattr(Image, algorithm.upper()) -> Image.BICUBIC etc.
    rescale_pil_algorithm = getattr(Image, algorithm.upper())
    pil_img = F.to_pil_image(samples[0].cpu())
    rescaled_pil = pil_img.resize((width, height), rescale_pil_algorithm)
    rescaled_tensor = F.to_tensor(rescaled_pil).unsqueeze(0)
    return rescaled_tensor.movedim(1, -1)

def rescale_m(samples, width, height, algorithm: str):
    """Rescales a mask tensor (BHW)."""
    samples = samples.unsqueeze(1)
    rescale_pil_algorithm = getattr(Image, algorithm.upper())
    pil_img = F.to_pil_image(samples[0].cpu())
    rescaled_pil = pil_img.resize((width, height), rescale_pil_algorithm)
    rescaled_tensor = F.to_tensor(rescaled_pil).unsqueeze(0)
    return rescaled_tensor.squeeze(1)

def blur_m(samples, pixels):
    """Blurs a mask tensor."""
    if pixels == 0:
        return samples
    mask = samples.squeeze(0)
    sigma = pixels / 4.0
    mask_np = mask.cpu().numpy()
    blurred_mask = gaussian_filter(mask_np, sigma=sigma)
    blurred_mask = torch.from_numpy(blurred_mask).float()
    blurred_mask = torch.clamp(blurred_mask, 0.0, 1.0)
    return blurred_mask.unsqueeze(0)

def expand_m(mask, pixels):
    """Expands a mask tensor using grey dilation."""
    if pixels == 0:
        return mask
    sigma = pixels / 4.0
    mask_np = mask.squeeze(0).cpu().numpy()
    kernel_size = math.ceil(sigma * 1.5 + 1)
    kernel = np.ones((kernel_size, kernel_size), dtype=np.uint8)
    dilated_mask = grey_dilation(mask_np, footprint=kernel)
    dilated_mask = torch.clamp(torch.from_numpy(dilated_mask.astype(np.float32)), 0.0, 1.0)
    return dilated_mask.unsqueeze(0)
    
def pad_to_multiple(value, multiple):
    """Calculates the padded value to be a multiple of a number."""
    return int(math.ceil(value / multiple) * multiple)

def crop_magic_im(image, mask, x, y, w, h, target_w, target_h, padding, downscale_algorithm, upscale_algorithm):
    """
    Core cropping logic. Creates a canvas, adjusts for aspect ratio,
    and returns the cropped sections and coordinates.
    """
    image = image.clone()
    mask = mask.clone()

    if target_w <= 0 or target_h <= 0 or w <= 0 or h <= 0:
        return image, 0, 0, image.shape[2], image.shape[1], image, mask, 0, 0, image.shape[2], image.shape[1]

    if padding > 1:
        target_w = pad_to_multiple(target_w, padding)
        target_h = pad_to_multiple(target_h, padding)

    target_aspect_ratio = target_w / target_h
    B, image_h, image_w, C = image.shape
    context_aspect_ratio = w / h

    new_x, new_y, new_w, new_h = x, y, w, h

    if context_aspect_ratio < target_aspect_ratio:
        new_w = int(h * target_aspect_ratio)
        new_x = x - (new_w - w) // 2
    else:
        new_h = int(w / target_aspect_ratio)
        new_y = y - (new_h - h) // 2
    
    # Simple boundary clamp for Detailer's purpose
    # The original has more complex logic to shift the box if it overflows,
    # but for Detailer, we just expand and let it create a canvas.
    
    up_padding = max(0, -new_y)
    down_padding = max(0, (new_y + new_h) - image_h)
    left_padding = max(0, -new_x)
    right_padding = max(0, (new_x + new_w) - image_w)

    expanded_image_h = image_h + up_padding + down_padding
    expanded_image_w = image_w + left_padding + right_padding
    
    canvas_image = torch.zeros((B, expanded_image_h, expanded_image_w, C), device=image.device)
    canvas_mask = torch.ones((B, expanded_image_h, expanded_image_w), device=mask.device)

    img_bchw = image.movedim(-1, 1)
    canvas_bchw = canvas_image.movedim(-1, 1)

    canvas_bchw[:, :, up_padding:up_padding + image_h, left_padding:left_padding + image_w] = img_bchw

    # Edge pixel padding
    if up_padding > 0:
        canvas_bchw[:, :, :up_padding, left_padding:left_padding + image_w] = img_bchw[:, :, 0:1, :]
    if down_padding > 0:
        canvas_bchw[:, :, -down_padding:, left_padding:left_padding + image_w] = img_bchw[:, :, -1:, :]
    if left_padding > 0:
        canvas_bchw[:, :, :, :left_padding] = canvas_bchw[:, :, :, left_padding:left_padding+1]
    if right_padding > 0:
        canvas_bchw[:, :, :, -right_padding:] = canvas_bchw[:, :, :, -right_padding-1:-right_padding]

    canvas_image = canvas_bchw.movedim(1, -1)
    canvas_mask[:, up_padding:up_padding + image_h, left_padding:left_padding + image_w] = mask

    cto_x, cto_y, cto_w, cto_h = left_padding, up_padding, image_w, image_h
    ctc_x, ctc_y, ctc_w, ctc_h = new_x + left_padding, new_y + up_padding, new_w, new_h

    cropped_image = canvas_image[:, ctc_y:ctc_y + ctc_h, ctc_x:ctc_x + ctc_w]
    cropped_mask = canvas_mask[:, ctc_y:ctc_y + ctc_h, ctc_x:ctc_x + ctc_w]
    
    rescale_algo = upscale_algorithm if target_w > ctc_w or target_h > ctc_h else downscale_algorithm
    
    final_cropped_image = rescale_i(cropped_image, target_w, target_h, rescale_algo)
    final_cropped_mask = rescale_m(cropped_mask, target_w, target_h, "nearest") # mask is always nearest

    return canvas_image, cto_x, cto_y, cto_w, cto_h, final_cropped_image, final_cropped_mask, ctc_x, ctc_y, ctc_w, ctc_h

def stitch_magic_im(canvas_image, inpainted_image, mask, ctc_x, ctc_y, ctc_w, ctc_h, cto_x, cto_y, cto_w, cto_h, downscale_algorithm, upscale_algorithm):
    """
    Core stitching logic. Resizes the inpainted result, blends it onto
    the canvas, and crops back to original dimensions.
    """
    canvas_image = canvas_image.clone()
    
    rescale_algo = upscale_algorithm if ctc_w > inpainted_image.shape[2] or ctc_h > inpainted_image.shape[1] else downscale_algorithm

    resized_image = rescale_i(inpainted_image, ctc_w, ctc_h, rescale_algo)
    # Mask should be rescaled with a smooth algorithm for good blending
    resized_mask = rescale_m(mask, ctc_w, ctc_h, "bicubic")

    resized_mask = resized_mask.clamp(0, 1).unsqueeze(-1)
    canvas_crop = canvas_image[:, ctc_y:ctc_y + ctc_h, ctc_x:ctc_x + ctc_w]

    blended = resized_mask * resized_image + (1.0 - resized_mask) * canvas_crop
    canvas_image[:, ctc_y:ctc_y + ctc_h, ctc_x:ctc_x + ctc_w] = blended

    output_image = canvas_image[:, cto_y:cto_y + cto_h, cto_x:cto_x + cto_w]
    return output_image

# endregion

class DetailerForEachMask:
    """
    –≠—Ç–∞ –Ω–æ–¥–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±–ª–∞—Å—Ç–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, —É–∫–∞–∑–∞–Ω–Ω—ã–µ –º–∞—Å–∫–∞–º–∏.
    –û–Ω–∞ –ø–µ—Ä–µ–±–∏—Ä–∞–µ—Ç –∫–∞–∂–¥—É—é –º–∞—Å–∫—É, –≤—ã—Ä–µ–∑–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –æ–±–ª–∞—Å—Ç—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º,
    –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Å–µ–º–ø–ª–µ—Ä –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏, –∞ –∑–∞—Ç–µ–º –≤—à–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ.
    –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ª–∏—Ü, –æ–±—ä–µ–∫—Ç–æ–≤ –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –¥–µ—Ç–∞–ª–µ–π —Å –ø–æ–º–æ—â—å—é BBOX –∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Å–æ–∫.
    –†–ï–§–ê–ö–¢–û–†–ò–ù–ì: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—É—é –ª–æ–≥–∏–∫—É –∫—Ä–æ–ø–∞/—Å—à–∏–≤–∫–∏ –∏–∑ –Ω–æ–¥—ã InpaintStitchImproved.
    """
    
    upscale_methods = ["nearest", "bilinear", "bicubic", "lanczos", "box", "hamming"]

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model": ("MODEL",),
                "positive": ("CONDITIONING",),
                "negative": ("CONDITIONING",),
                "vae": ("VAE",),
                "image": ("IMAGE", {"tooltip": "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—Ö–æ–ª—Å—Ç), –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å—Å—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è."}),
                "masks": ("MASK", {"tooltip": "–ú–∞—Å–∫–∞ —Å –æ–¥–Ω–æ–π –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –æ–±–ª–∞—Å—Ç—è–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç BBOX) –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏."}),

                # Sampler settings
                "noise_seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "tooltip": "–ù–∞—á–∞–ª—å–Ω–æ–µ —Å–ª—É—á–∞–π–Ω–æ–µ –∑–µ—Ä–Ω–æ –¥–ª—è —à—É–º–∞. –ë—É–¥–µ—Ç —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å—Å—è –Ω–∞ 1 –¥–ª—è –∫–∞–∂–¥–æ–π —Å–ª–µ–¥—É—é—â–µ–π –º–∞—Å–∫–∏."}),
                "steps": ("INT", {"default": 20, "min": 1, "max": 10000, "tooltip": "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –æ–±–ª–∞—Å—Ç–∏."}),
                "cfg": ("FLOAT", {"default": 8.0, "min": 0.0, "max": 100.0, "step":0.1, "tooltip": "–°–∏–ª–∞ –≤–ª–∏—è–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (Classifier-Free Guidance)."}),
                "sampler_name": (comfy.samplers.KSampler.SAMPLERS, {"tooltip": "–ê–ª–≥–æ—Ä–∏—Ç–º —Å–µ–º–ø–ª–µ—Ä–∞, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏."}),
                "scheduler": (comfy.samplers.KSampler.SCHEDULERS, {"tooltip": "–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —à–∞–≥–æ–≤ –¥–ª—è —Å–µ–º–ø–ª–µ—Ä–∞."}),
                "denoise": ("FLOAT", {"default": 0.4, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "–°–∏–ª–∞ –æ–±–µ—Å—à—É–º–ª–∏–≤–∞–Ω–∏—è. 1.0 ‚Äî –ø–æ–ª–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–∏, <1.0 ‚Äî —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–∏ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã."}),

                # Crop & Stitch settings
                "context_expand_pixels": ("INT", {"default": 32, "min": 0, "max": 512, "step": 8, "tooltip": "–ù–∞ —Å–∫–æ–ª—å–∫–æ –ø–∏–∫—Å–µ–ª–µ–π —Ä–∞—Å—à–∏—Ä–∏—Ç—å –æ–±–ª–∞—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ –∫–∞–∂–¥–æ–π –º–∞—Å–∫–∏."}),
                "blur_mask_pixels": ("FLOAT", {"default": 8.0, "min": 0.0, "max": 64.0, "step": 0.1, "tooltip": "–†–∞–∑–º—ã—Ç–∏–µ –º–∞—Å–∫–∏ –¥–ª—è –∏–Ω–ø–µ–π–Ω—Ç–∏–Ω–≥–∞ (–¥–æ VAE). –ü–æ–º–æ–≥–∞–µ—Ç —Å–æ–∑–¥–∞—Ç—å –±–æ–ª–µ–µ –ø–ª–∞–≤–Ω—ã–µ –∫—Ä–∞—è –≤ –ª–∞—Ç–µ–Ω—Ç–µ."}),
                "blend_pixels": ("FLOAT", {"default": 8.0, "min": 0.0, "max": 64.0, "step": 0.1, "tooltip": "–†–∞–¥–∏—É—Å —Ä–∞–∑–º—ã—Ç–∏—è –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ —Å–º–µ—à–∏–≤–∞–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —É—á–∞—Å—Ç–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º."}),
                "grow_mask_by": ("INT", {"default": 6, "min": 0, "max": 64, "step": 1, "tooltip": "–ù–∞ —Å–∫–æ–ª—å–∫–æ –ø–∏–∫—Å–µ–ª–µ–π —Ä–∞—Å—à–∏—Ä–∏—Ç—å –º–∞—Å–∫—É –≤ –ª–∞—Ç–µ–Ω—Ç–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∂–µ—Å—Ç–∫–∏—Ö –∫—Ä–∞–µ–≤."}),
                
                # Rescale settings
                "force_width": ("INT", {"default": 512, "min": 0, "max": nodes.MAX_RESOLUTION, "step": 8, "tooltip": "–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è. 0 = –∞–≤—Ç–æ."}),
                "force_height": ("INT", {"default": 512, "min": 0, "max": nodes.MAX_RESOLUTION, "step": 8, "tooltip": "–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –≤—ã—Å–æ—Ç–∞ –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è. 0 = –∞–≤—Ç–æ."}),
                "downscale_algorithm": (cls.upscale_methods, {"default": "bilinear", "tooltip": "–ê–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∞."}),
                "upscale_algorithm": (cls.upscale_methods, {"default": "bicubic", "tooltip": "–ê–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –º–∞—Å—à—Ç–∞–±–∞."}),
                "padding": ([8, 16, 32, 64, 128, 256], {"default": 32, "tooltip": "–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –≤—ã—Ä–µ–∑–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏. –ï–µ —à–∏—Ä–∏–Ω–∞ –∏ –≤—ã—Å–æ—Ç–∞ –±—É–¥—É—Ç –∫—Ä–∞—Ç–Ω—ã —ç—Ç–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é."}),

                # Mask processing order
                "mask_process_order": (["—Å–≤–µ—Ä—Ö—É-–≤–Ω–∏–∑", "—Å–Ω–∏–∑—É-–≤–≤–µ—Ä—Ö", "—Å–ª–µ–≤–∞-–Ω–∞–ø—Ä–∞–≤–æ", "—Å–ø—Ä–∞–≤–∞-–Ω–∞–ª–µ–≤–æ", "–æ—Ç –±–æ–ª—å—à–µ–π –∫ –º–µ–Ω—å—à–µ–π", "–æ—Ç –º–µ–Ω—å—à–µ–π –∫ –±–æ–ª—å—à–µ–π", "—Å–ª—É—á–∞–π–Ω–æ"],
                                       {"default": "—Å–≤–µ—Ä—Ö—É-–≤–Ω–∏–∑", "tooltip": "–ü–æ—Ä—è–¥–æ–∫, –≤ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥—É—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –º–∞—Å–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ."}),
            }
        }

    RETURN_TYPES = ("IMAGE", "LATENT", "MASK")
    RETURN_NAMES = ("image", "latent", "processed_masks")
    FUNCTION = "detail_sequentially"
    CATEGORY = "üòé SnJake/Detailer"

    def detail_sequentially(self, model, positive, negative, vae, image, masks,
                            noise_seed, steps, cfg, sampler_name, scheduler, denoise,
                            context_expand_pixels, blur_mask_pixels, blend_pixels, grow_mask_by,
                            force_width, force_height, downscale_algorithm, upscale_algorithm, padding,
                            mask_process_order):
        
        if masks.numel() == 0 or masks.max() == 0:
            print("–ú–∞—Å–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: VAE –æ–∂–∏–¥–∞–µ—Ç BCHW, –∞ –Ω–µ BHWC
            final_latent_tensor = vae.encode(image[:,:,:,:3].movedim(-1, 1))
            return (image, {"samples": final_latent_tensor}, torch.zeros_like(masks))

        mask_np = masks.cpu().numpy().squeeze()
        labeled_array, num_features = label(mask_np > 0.5)
        if num_features == 0:
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: VAE –æ–∂–∏–¥–∞–µ—Ç BCHW, –∞ –Ω–µ BHWC
            final_latent_tensor = vae.encode(image[:,:,:,:3].movedim(-1, 1))
            return (image, {"samples": final_latent_tensor}, torch.zeros_like(masks))
            
        found_objects = find_objects(labeled_array)
        decorated_masks = [{'slice': slc, 'label': i + 1, 'area': np.sum(labeled_array[slc] == i + 1),
                            'center_y': slc[0].start + (slc[0].stop - slc[0].start) / 2,
                            'center_x': slc[1].start + (slc[1].stop - slc[1].start) / 2}
                           for i, slc in enumerate(found_objects) if slc is not None]

        sort_key_map = {"—Å–ª–µ–≤–∞-–Ω–∞–ø—Ä–∞–≤–æ": "center_x", "—Å–ø—Ä–∞–≤–∞-–Ω–∞–ª–µ–≤–æ": "center_x", "—Å–≤–µ—Ä—Ö—É-–≤–Ω–∏–∑": "center_y", "—Å–Ω–∏–∑—É-–≤–≤–µ—Ä—Ö": "center_y", "–æ—Ç –±–æ–ª—å—à–µ–π –∫ –º–µ–Ω—å—à–µ–π": "area", "–æ—Ç –º–µ–Ω—å—à–µ–π –∫ –±–æ–ª—å—à–µ–π": "area"}
        if mask_process_order in sort_key_map:
            decorated_masks.sort(key=lambda m: m[sort_key_map[mask_process_order]], reverse=(mask_process_order in {"—Å–ø—Ä–∞–≤–∞-–Ω–∞–ª–µ–≤–æ", "—Å–Ω–∏–∑—É-–≤–≤–µ—Ä—Ö", "–æ—Ç –±–æ–ª—å—à–µ–π –∫ –º–µ–Ω—å—à–µ–π"}))
        elif mask_process_order == "—Å–ª—É—á–∞–π–Ω–æ":
            import random
            random.shuffle(decorated_masks)

        image_to_process = image.clone()
        original_height, original_width = image.shape[1], image.shape[2]
        final_processed_mask = torch.zeros((1, original_height, original_width), device=image.device)
        pbar = comfy.utils.ProgressBar(num_features)

        for i, mask_info in enumerate(decorated_masks):
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å–∫–∏ {i+1}/{num_features}...")
            
            current_mask_np = (labeled_array == mask_info['label']).astype(np.float32)
            current_mask_tensor = torch.from_numpy(current_mask_np).to(image.device).unsqueeze(0)
            
            if blur_mask_pixels > 0:
                inpaint_mask = blur_m(current_mask_tensor, blur_mask_pixels)
            else:
                inpaint_mask = current_mask_tensor

            slc = mask_info['slice']
            y_min, y_max = slc[0].start, slc[0].stop
            x_min, x_max = slc[1].start, slc[1].stop

            x = x_min - context_expand_pixels
            y = y_min - context_expand_pixels
            w = (x_max - x_min) + 2 * context_expand_pixels
            h = (y_max - y_min) + 2 * context_expand_pixels

            target_w = force_width if force_width > 0 else w
            target_h = force_height if force_height > 0 else h

            canvas_image, cto_x, cto_y, cto_w, cto_h, \
            cropped_image, cropped_mask, \
            ctc_x, ctc_y, ctc_w, ctc_h = crop_magic_im(
                image_to_process, inpaint_mask, x, y, w, h,
                target_w, target_h, padding, downscale_algorithm, upscale_algorithm
            )
            
            if cropped_image.shape[1] == 0 or cropped_image.shape[2] == 0: continue

            latent_mask_for_inpaint = cropped_mask.clone()
            pixels_for_concat = cropped_image * (1.0 - latent_mask_for_inpaint.round().unsqueeze(-1))
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: vae.encode() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç BCHW, –≤—Ç–æ—Ä–æ–π movedim –Ω–µ –Ω—É–∂–µ–Ω
            concat_latent = vae.encode(pixels_for_concat.movedim(-1, 1))
            initial_latent_samples = vae.encode(cropped_image.movedim(-1, 1))

            latent_for_sampler = {"samples": initial_latent_samples}
            mask_for_sampler = latent_mask_for_inpaint.reshape((-1, 1, latent_mask_for_inpaint.shape[-2], latent_mask_for_inpaint.shape[-1]))
            
            if grow_mask_by > 0:
                grown_mask = expand_m(mask_for_sampler.squeeze(0), grow_mask_by*2).unsqueeze(0)
            else:
                grown_mask = mask_for_sampler

            latent_h, latent_w = initial_latent_samples.shape[2], initial_latent_samples.shape[3]
            latent_for_sampler["noise_mask"] = torch.nn.functional.interpolate(grown_mask, size=(latent_h, latent_w), mode="bilinear")

            positive_inpaint = [[c[0], {**c[1], 'concat_latent_image': concat_latent, 'concat_mask': mask_for_sampler}] for c in positive]
            negative_inpaint = [[c[0], {**c[1], 'concat_latent_image': concat_latent, 'concat_mask': mask_for_sampler}] for c in negative]
            
            latent_out = nodes.common_ksampler(model, noise_seed, steps, cfg, sampler_name, scheduler, positive_inpaint, negative_inpaint, latent_for_sampler, denoise=denoise)[0]
            noise_seed += 1
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: latent_out["samples"] —É–∂–µ BCHW. vae.decode() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç BCHW, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–¥–æ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –≤ BHWC.
            decoded_crop = vae.decode(latent_out["samples"]).movedim(1, -1)

            blend_mask = blur_m(cropped_mask, blend_pixels)
            
            image_to_process = stitch_magic_im(
                canvas_image, decoded_crop, blend_mask,
                ctc_x, ctc_y, ctc_w, ctc_h,
                cto_x, cto_y, cto_w, cto_h,
                downscale_algorithm, upscale_algorithm
            )
            
            temp_mask_canvas = torch.zeros_like(canvas_image[:,:,:,0])
            blend_mask_resized = rescale_m(blend_mask, ctc_w, ctc_h, "bicubic")
            temp_mask_canvas[:, ctc_y:ctc_y+ctc_h, ctc_x:ctc_x+ctc_w] = blend_mask_resized
            processed_part_mask = temp_mask_canvas[:, cto_y:cto_y+cto_h, cto_x:cto_x+cto_w]
            
            final_processed_mask = torch.max(final_processed_mask, processed_part_mask)
            pbar.update(1)
        
        final_processed_mask.clamp_(0.0, 1.0)
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: VAE –æ–∂–∏–¥–∞–µ—Ç BCHW. –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω—É–∂–Ω–æ –æ–±–µ—Ä–Ω—É—Ç—å –≤ —Å–ª–æ–≤–∞—Ä—å.
        final_latent_tensor = vae.encode(image_to_process[:,:,:,:3].movedim(-1, 1))
        
        return (image_to_process, {"samples": final_latent_tensor}, final_processed_mask.squeeze(0))
